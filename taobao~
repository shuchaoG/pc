# -*- coding:utf-8 -*-

import urllib
import urllib2
import re
import tool


class Spider:
     #页面初始化
    def __init__(self):
        self.siteURL = 'http://mm.taobao.com/json/request_top_list.htm'
	self.tool=tool.Tool()
    
     #获取页面信息
    def getPage(self,pageIndex):
        url = self.siteURL + "?page=" + str(pageIndex)
        #print url,u"你好"
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
        return response.read().decode('gbk')

    #获取页面所有MM信息，list格式
    def getContents(self,pageIndex):
        page = self.getPage(pageIndex)
        pattern = re.compile('<div class="list-item".*?pic-word.*?<a href="(.*?)".*?<img src="(.*?)".*?<a class="lady-name.*?>(.*?)</a>.*?<strong>(.*?)</strong>.*?<span>(.*?)</span>',re.S)
        items = re.findall(pattern,page)
	contents=[]
        for item in items:
            contents.append([item[0],item[1],item[2],item[3],item[4]])	
            #print item[0],item[1],item[2]
	return contents

    #获取个人详情页
    def getDetailPage(self,infoURL):
        response = urllib2.urlopen(infoURL)
        return response.read().decode('gbk')    

    #获取个人文字简介
    def getBrief(self,page):
        pattern = re.compile('<div class="mm-aixiu-content".*?>(.*?)<!--',re.S)
        result = re.search(pattern,page)
        return self.tool.replace(result.group(1))

    #获取页面所有图片
    def getAllImg(self,page):
        pattern = re.compile('<div class="mm-aixiu-content".*?>(.*?)<!--',re.S)
        #个人信息页所有代码
        content = re.search(pattern,page)
        #代码中的所有图片
        patternImg = re.compile('<img.*?src="(.*?)"',re.S)
        images = re.findall(patternImg,content.group(1))
        return images


    #保存多张图片
    def saveImgs(self,images,name):
        number = 1
        print u"发现",name,u"共有",len(images),u"多少张"
        for imageURL in images:
            splitPath = imageURL.split('.')
            fTail = splitPath.pop()
            if len(fTail) > 3:
                fTail = "jpg"
            fileName = name + "/" + str(number) + "." + fTail
            self.saveImg(imageURL,fileName)
            number += 1
 
    #保存头像
    def saveIcon(self,iconURL,name):
        splitPath = iconURL.split('.')
        fTail = splitPath.pop()
        fileName = name + "/icon." + fTail
        self.saveImg(iconURL,fileName)
 
    #保存个人简介
    def saveBrief(self,content,name):
        fileName = name + "/" + name + ".txt"
        f = open(fileName,"w+")
        print u"正在保存个人简介",fileName
        f.write(content.encode('utf-8'))
 
 
    #传入图片地址，文件名，保存图片
    def saveImg(self,imageURL,fileName):
         u = urllib.urlopen(imageURL)
         data = u.read()
         f = open(fileName, 'wb')
         f.write(data)
         print u"正在保存图片",fileName
         f.close()
 
    #创建新目录
    def mkdir(self,path):
        path = path.strip()
        # 判断路径是否存在
        # 存在    True
        # 不存在  False
        isExists=os.path.exists(path)
        # 判断结果
        if not isExists:
            # 如果不存在则创建新目录
            print u"创建了",path,u'文件'
            # 创建目录的函数
            os.makedirs(path)
            return True
        else:
            # 如果存在则不创建，并且提示目录存在
            print u"存在",path,'的文件'
            return False
 
    #将一页淘宝MM的信息保存下来
    def savePageInfo(self,pageIndex):
        #获取第一页MM的信息
        contents = self.getContents(pageIndex)
        for item in contents:
            #item[0]个人详情URL,item[1]头像URL,item[2]姓名,item[3]年龄,item[4]居住地
            print u"一位模特,她名叫",item[2],u"年龄",item[3],u",居住地",item[4]
            print u"正在保存",item[2],"的信息"
            print u"发现她的个人地址是",item[0]
            #个人详情URL
            detailURL = item[0]
            #个人详情页面代码
            detailPage = self.getDetailPage(detailURL)
            #获取个人简介
            brief = self.getBrief(detailPage)
            #获取所有图片
            images = self.getAllImg(detailPage)
            self.mkdir(item[2])
            #保存个人简介
            self.saveBrief(brief,item[2])
            #保存图标
            self.saveIcon(item[1],item[2])
            #保存图片
            self.saveImgs(images,item[2])
 
    #传入起始和终止页码
    def savePagesInfo(self,start,end):
        for i in range(start,end+1):
            print u"正在查找",i,u"个MM"
            self.savePageInfo(i)
 
 
#传入起始和终止页码2和10
spider = Spider()
spider.savePagesInfo(2,10)



